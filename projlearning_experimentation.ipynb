{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from copy import deepcopy\n",
    "#imdb = keras.datasets.imdb\n",
    "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ssome of the parameters\n",
    "d = 5 #d\n",
    "k = 6 #k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_iter(examples, transforms, b, learning_rate=0.1):\n",
    "    '''\n",
    "    Runs one iteration through all provided examples\n",
    "    '''\n",
    "    total_loss = 0\n",
    "    for q,h,t in examples:\n",
    "        # project the query with all of the projection matrices\n",
    "        \n",
    "        p = [np.dot(transforms[i],q).T for i in range(k)]\n",
    "        \n",
    "        \n",
    "        # Normalize the projections\n",
    "        for i in range(k):\n",
    "            p[i] = p[i] / np.sqrt((np.sum(p[i]**2)))\n",
    "        \n",
    "        # compute similarities between all projections and the candidate\n",
    "        s = np.dot(p,h)\n",
    "        \n",
    "        # Apply sigmoid function and find the transformation that resulted\n",
    "        # in the closest projection to the candidate\n",
    "        sigmoids = sigmoid(np.add(s,b))\n",
    "        maxsim = np.argmax(sigmoids)\n",
    "        \n",
    "        # y: predicted similarity\n",
    "        # x: the corresponding projected vector \n",
    "        y = sigmoids[maxsim]\n",
    "        x = p[maxsim]\n",
    "        \n",
    "        if(y == 0 or y==1):\n",
    "            print(\"Perfect hit\")\n",
    "            continue\n",
    "        # Compute the loss and update the corresponding projection matrix\n",
    "        # in accordance with gradient descent.\n",
    "        loss = t*np.log(y) + (np.subtract(1,t)*np.log(np.subtract(1,y)))\n",
    "        total_loss += loss\n",
    "        \n",
    "        gradient = np.dot(x.T,loss)\n",
    "        \n",
    "        #print(\"Grad: \",gradient,\"\\n\")\n",
    "        #print(\"Grad*lr: \", np.multiply(learning_rate,gradient))\n",
    "        prev_theta = transforms[maxsim]\n",
    "        #print(\"Before: \", prev_theta)\n",
    "        transforms[maxsim] = np.subtract(prev_theta, np.multiply(learning_rate,gradient))\n",
    "        #print(\"OLD: \", prev_theta ,\"\\n\", \"NEW: \", transforms[maxsim],\"\\n\")\n",
    "        #print(\"After: \", transforms[maxsim],\"\\n\")\n",
    "        #Update the bias\n",
    "        b_loss = y + (b[maxsim]-t)\n",
    "        prev_b = b[maxsim]\n",
    "        b[maxsim] =  np.subtract(prev_b, np.multiply(learning_rate, b_loss))\n",
    "    return total_loss\n",
    "        \n",
    "def train(embeddings, train_examples, num_iter=1000):\n",
    "    '''\n",
    "    The main training algorithm. \n",
    "    '''\n",
    "    transforms = [] #phi\n",
    "    for i in range(k):\n",
    "        transforms.append(np.identity(d) +np.random.normal(0, 0.1, [d,d]))\n",
    "    b = np.zeros(k)  #bias\n",
    "    \n",
    "    for it in range(num_iter):\n",
    "        loss = one_iter(train_examples, transforms,b)\n",
    "        print(\"Loss, iter: \", loss/len(train_examples), it, \"\\n\")\n",
    "    #print(b)\n",
    "    return (transforms, b)\n",
    "    \n",
    "def extract(q, transforms, embeddings, b, n=3):\n",
    "    '''\n",
    "    Extract the n top ranked candidates\n",
    "    '''\n",
    "    #print(\"Q: \", q,\"\\n\")\n",
    "    # p: All projections of q, list of k vectors\n",
    "    p = [np.dot(transforms[i],q).T for i in range(k)]\n",
    "    \n",
    "    # Normalize all projections\n",
    "    for i in range(k):\n",
    "        p[i] = p[i] / np.sqrt((np.sum(p[i]**2)))\n",
    "    \n",
    "    # s: similarities of all projections of q, with all other terms\n",
    "    s = [(np.dot(p,embeddings[h]), h) for h in embeddings]\n",
    "    \n",
    "    #print(\"Projections: \",p,\"\\n\")\n",
    "    #print(\"embeddings: \", embeddings,\"\\n\")\n",
    "    maxsims = [(_s[np.argmax(_s)], np.argmax(_s), h) for _s,h in s]\n",
    "    #print(s)\n",
    "    sigmoids = [(sigmoid(np.add(_s,b[idx])), h) for _s,idx,h in maxsims]\n",
    "    sigmoids.sort()\n",
    "    return sigmoids[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([ 0.19900744,  0.59702231,  0.59702231, -0.39801488, -0.29851116]), 1), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([ 0.44232587,  0.58976782, -0.58976782,  0.14744196,  0.29488391]), 1), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([ 0.1796053 ,  0.53881591,  0.3592106 , -0.1796053 , -0.71842121]), 1), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([ 0.91499142, -0.15249857, -0.15249857, -0.15249857, -0.30499714]), 0), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([-0.85714286,  0.14285714, -0.28571429, -0.28571429,  0.28571429]), 0), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([ 0.        ,  0.11470787, -0.57353933,  0.57353933, -0.57353933]), 0), (array([-0.18731716, -0.65561007,  0.56195149,  0.28097574,  0.37463432]), array([0.08304548, 0.49827288, 0.49827288, 0.49827288, 0.49827288]), 0), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([ 0.81649658,  0.27216553,  0.13608276, -0.27216553, -0.40824829]), 1), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([-0.78113347,  0.26037782, -0.39056673,  0.13018891,  0.39056673]), 1), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([ 0.        ,  0.11470787, -0.57353933,  0.57353933, -0.57353933]), 0), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([0.        , 0.10101525, 0.50507627, 0.60609153, 0.60609153]), 0), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([ 0.20965697, -0.6289709 , -0.52414242, -0.10482848, -0.52414242]), 0), (array([-0.83205029,  0.2773501 , -0.2773501 ,  0.2773501 ,  0.2773501 ]), array([ 0.        ,  0.        ,  0.14744196, -0.88465174, -0.44232587]), 0)]\n"
     ]
    }
   ],
   "source": [
    "# Read in testing environment\n",
    "em = False\n",
    "test_embeddings = {}\n",
    "train_examples = []\n",
    "testfile = \"/home/johannes/thesis_code/ml_experimentation/test_setup.txt\"\n",
    "\n",
    "fp = open(testfile,'r')\n",
    "for line in fp:\n",
    "    \n",
    "    if(\"Embeddings\" in line):\n",
    "        #print(line)\n",
    "        em = True\n",
    "        continue\n",
    "    if not line or line.startswith(\"#\"):\n",
    "        continue\n",
    "    if em:\n",
    "        emb = np.array([int(i) for i in line.split()[1:]])\n",
    "        test_embeddings[line.split()[0]] = emb\n",
    "    else:\n",
    "        q,h,t = line.split() \n",
    "        train_examples.append((q,h,int(t)))\n",
    "\n",
    "for w in test_embeddings:\n",
    "    test_embeddings[w] = test_embeddings[w] / np.sqrt((np.sum(test_embeddings[w]**2)))\n",
    "    \n",
    "train_embeddings = [(test_embeddings[q],test_embeddings[h],t) for q,h,t in train_examples]\n",
    "print(train_embeddings)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, iter:  -0.7826396667626596 0 \n",
      "\n",
      "Loss, iter:  -0.747866535752945 1 \n",
      "\n",
      "Loss, iter:  -0.7336575310923354 2 \n",
      "\n",
      "Loss, iter:  -0.7174338182067531 3 \n",
      "\n",
      "Loss, iter:  -0.7100383615494041 4 \n",
      "\n",
      "Loss, iter:  -0.7035941169625203 5 \n",
      "\n",
      "Loss, iter:  -0.6947262642331737 6 \n",
      "\n",
      "Loss, iter:  -0.689150373593784 7 \n",
      "\n",
      "Loss, iter:  -0.6846491623421993 8 \n",
      "\n",
      "Loss, iter:  -0.6813568258323931 9 \n",
      "\n",
      "Loss, iter:  -0.679846285195199 10 \n",
      "\n",
      "Loss, iter:  -0.6767688819620107 11 \n",
      "\n",
      "Loss, iter:  -0.6732215168280995 12 \n",
      "\n",
      "Loss, iter:  -0.6726837424067694 13 \n",
      "\n",
      "Loss, iter:  -0.6708533945822608 14 \n",
      "\n",
      "Loss, iter:  -0.6677654562682035 15 \n",
      "\n",
      "Loss, iter:  -0.6691663757372718 16 \n",
      "\n",
      "Loss, iter:  -0.6689909238993574 17 \n",
      "\n",
      "Loss, iter:  -0.6700886586452846 18 \n",
      "\n",
      "Loss, iter:  -0.6669218547791094 19 \n",
      "\n",
      "Loss, iter:  -0.669165735888176 20 \n",
      "\n",
      "Loss, iter:  -0.6654126828108695 21 \n",
      "\n",
      "Loss, iter:  -0.6682461093173692 22 \n",
      "\n",
      "Loss, iter:  -0.6658958308882367 23 \n",
      "\n",
      "Loss, iter:  -0.6675609902951382 24 \n",
      "\n",
      "Loss, iter:  -0.6655273691310768 25 \n",
      "\n",
      "Loss, iter:  -0.6631333826372658 26 \n",
      "\n",
      "Loss, iter:  -0.665051953934742 27 \n",
      "\n",
      "Loss, iter:  -0.66518395169148 28 \n",
      "\n",
      "Loss, iter:  -0.6644009792638378 29 \n",
      "\n",
      "Loss, iter:  -0.6643265187346837 30 \n",
      "\n",
      "Loss, iter:  -0.6641845426891421 31 \n",
      "\n",
      "Loss, iter:  -0.6625166638849329 32 \n",
      "\n",
      "Loss, iter:  -0.6651204753082163 33 \n",
      "\n",
      "Loss, iter:  -0.6633443997543532 34 \n",
      "\n",
      "Loss, iter:  -0.6584271859474027 35 \n",
      "\n",
      "Loss, iter:  -0.6654135238495038 36 \n",
      "\n",
      "Loss, iter:  -0.6632032467215573 37 \n",
      "\n",
      "Loss, iter:  -0.6583264835759361 38 \n",
      "\n",
      "Loss, iter:  -0.6623274824975952 39 \n",
      "\n",
      "Loss, iter:  -0.658829686889974 40 \n",
      "\n",
      "Loss, iter:  -0.6588594340222665 41 \n",
      "\n",
      "Loss, iter:  -0.6621539647954857 42 \n",
      "\n",
      "Loss, iter:  -0.6571299205372878 43 \n",
      "\n",
      "Loss, iter:  -0.6626339683061209 44 \n",
      "\n",
      "Loss, iter:  -0.6640371862655239 45 \n",
      "\n",
      "Loss, iter:  -0.6621055928528204 46 \n",
      "\n",
      "Loss, iter:  -0.6572689185405446 47 \n",
      "\n",
      "Loss, iter:  -0.661267944662185 48 \n",
      "\n",
      "Loss, iter:  -0.6578794611270136 49 \n",
      "\n",
      "Loss, iter:  -0.6579516413630053 50 \n",
      "\n",
      "Loss, iter:  -0.6579852213568308 51 \n",
      "\n",
      "Loss, iter:  -0.6580089754936995 52 \n",
      "\n",
      "Loss, iter:  -0.6580299960073374 53 \n",
      "\n",
      "Loss, iter:  -0.6580489775809517 54 \n",
      "\n",
      "Loss, iter:  -0.6580653524525741 55 \n",
      "\n",
      "Loss, iter:  -0.6580786120364361 56 \n",
      "\n",
      "Loss, iter:  -0.6580885200421445 57 \n",
      "\n",
      "Loss, iter:  -0.6580950624898889 58 \n",
      "\n",
      "Loss, iter:  -0.6566105973987539 59 \n",
      "\n",
      "Loss, iter:  -0.660715788710615 60 \n",
      "\n",
      "Loss, iter:  -0.6570764150846424 61 \n",
      "\n",
      "Loss, iter:  -0.6572227527661892 62 \n",
      "\n",
      "Loss, iter:  -0.657306705135508 63 \n",
      "\n",
      "Loss, iter:  -0.6573653769695336 64 \n",
      "\n",
      "Loss, iter:  -0.6574116466857691 65 \n",
      "\n",
      "Loss, iter:  -0.6574499714301753 66 \n",
      "\n",
      "Loss, iter:  -0.657482115494566 67 \n",
      "\n",
      "Loss, iter:  -0.6575089783357908 68 \n",
      "\n",
      "Loss, iter:  -0.6575311609721202 69 \n",
      "\n",
      "Loss, iter:  -0.6575491420756323 70 \n",
      "\n",
      "Loss, iter:  -0.6575633346753995 71 \n",
      "\n",
      "Loss, iter:  -0.6575741062758459 72 \n",
      "\n",
      "Loss, iter:  -0.6575817875785781 73 \n",
      "\n",
      "Loss, iter:  -0.6575866774726873 74 \n",
      "\n",
      "Loss, iter:  -0.6575890466420288 75 \n",
      "\n",
      "Loss, iter:  -0.6575891405261045 76 \n",
      "\n",
      "Loss, iter:  -0.6575871818817055 77 \n",
      "\n",
      "Loss, iter:  -0.6575833730425487 78 \n",
      "\n",
      "Loss, iter:  -0.6575778979267958 79 \n",
      "\n",
      "Loss, iter:  -0.6575709238259585 80 \n",
      "\n",
      "Loss, iter:  -0.6583627769407089 81 \n",
      "\n",
      "Loss, iter:  -0.662191490736623 82 \n",
      "\n",
      "Loss, iter:  -0.6580840333284537 83 \n",
      "\n",
      "Loss, iter:  -0.657736578355932 84 \n",
      "\n",
      "Loss, iter:  -0.657530443151668 85 \n",
      "\n",
      "Loss, iter:  -0.6574096487169245 86 \n",
      "\n",
      "Loss, iter:  -0.6573393541468486 87 \n",
      "\n",
      "Loss, iter:  -0.6572984521971114 88 \n",
      "\n",
      "Loss, iter:  -0.6572743428428329 89 \n",
      "\n",
      "Loss, iter:  -0.6572595727677896 90 \n",
      "\n",
      "Loss, iter:  -0.6572497667034095 91 \n",
      "\n",
      "Loss, iter:  -0.6572423792716146 92 \n",
      "\n",
      "Loss, iter:  -0.6572359488727954 93 \n",
      "\n",
      "Loss, iter:  -0.656026190371555 94 \n",
      "\n",
      "Loss, iter:  -0.6618976058667307 95 \n",
      "\n",
      "Loss, iter:  -0.6622376048347876 96 \n",
      "\n",
      "Loss, iter:  -0.6567534837966666 97 \n",
      "\n",
      "Loss, iter:  -0.6600536869184152 98 \n",
      "\n",
      "Loss, iter:  -0.6567249463219833 99 \n",
      "\n",
      "[-0.19096457 -0.20590735 -0.24389482 -0.13546904 -0.20221739 -0.18290347]\n"
     ]
    }
   ],
   "source": [
    "transforms,b = train(test_embeddings, train_embeddings, 100)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.28652179431894764, 'music'), (0.324133318600194, 'boi'), (0.42408666662429156, 'food'), (0.43308712770243246, 'fruit'), (0.43586818686745243, 'cat'), (0.46230734498396053, 'brand'), (0.4800638221862114, 'car'), (0.5027621490635844, 'vehichle'), (0.5048742696314579, 'metallica'), (0.5169823487957836, 'volvo'), (0.5257952339801082, 'band'), (0.5890236012185236, 'group'), (0.6767078147867991, 'horse'), (0.7013302634092692, 'pelle')]\n"
     ]
    }
   ],
   "source": [
    "query = test_embeddings[\"volvo\"]\n",
    "\n",
    "predictions = extract(query, transforms, test_embeddings, b,15)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
